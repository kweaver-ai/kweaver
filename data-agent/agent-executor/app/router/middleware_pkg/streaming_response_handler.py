# -*- coding:utf-8 -*-
"""
æµå¼å“åº”å¤„ç†æ¨¡å—
è´Ÿè´£æµå¼å“åº”çš„æ‹¦æˆªã€è®°å½•å’Œæ–‡ä»¶å­˜å‚¨
"""

import os
import time
from datetime import datetime

from app.common.config import Config
from app.common.stand_log import StandLogger
from app.common.struct_logger import struct_logger
from .streaming_rate_limiter import create_rate_limited_iterator

# æµå¼å“åº”æ—¥å¿—å­˜å‚¨ç›®å½•
STREAMING_RESPONSE_LOG_DIR = "log/streaming_responses"

# è·å–è¯·æ±‚æ—¥å¿—è®°å½•å™¨
request_logger = StandLogger.get_request_logger()


def _ensure_streaming_log_dir():
    """ç¡®ä¿æµå¼å“åº”æ—¥å¿—ç›®å½•å­˜åœ¨"""
    if not os.path.exists(STREAMING_RESPONSE_LOG_DIR):
        os.makedirs(STREAMING_RESPONSE_LOG_DIR, exist_ok=True)


def _get_streaming_log_file_path(request_id: str) -> str:
    """è·å–æµå¼å“åº”æ—¥å¿—æ–‡ä»¶è·¯å¾„

    Args:
        request_id (str): è¯·æ±‚ID

    Returns:
        str: æ—¥å¿—æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    """
    # ä½¿ç”¨æ—¶é—´æˆ³å’Œrequest_idåˆ›å»ºæ–‡ä»¶åï¼Œé¿å…å†²çª
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}_{request_id}.log"
    return os.path.join(STREAMING_RESPONSE_LOG_DIR, filename)


def _write_chunk_to_file(
    file_path: str, chunk_content: str, chunk_number: int, chunk_size: int
):
    """å°†æµå¼å“åº”å—å†™å…¥æ–‡ä»¶

    Args:
        file_path (str): æ—¥å¿—æ–‡ä»¶è·¯å¾„
        chunk_content (str): å—å†…å®¹
        chunk_number (int): å—åºå·
        chunk_size (int): å—å¤§å°ï¼ˆå­—èŠ‚æ•°ï¼‰
    """
    try:
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(
                f"[{datetime.now().isoformat()}] Chunk {chunk_number} ({chunk_size} bytes):\n"
            )
            f.write(chunk_content)
            f.write("\n" + "=" * 50 + "\n")
    except Exception as e:
        struct_logger.console_logger.error(
            f"å†™å…¥æµå¼å“åº”æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}",
            file_path=file_path,
            chunk_number=chunk_number,
        )


def _write_stream_completion_info(file_path: str, chunks_count: int, total_bytes: int):
    """å†™å…¥æµå¼å“åº”å®Œæˆä¿¡æ¯

    Args:
        file_path (str): æ—¥å¿—æ–‡ä»¶è·¯å¾„
        chunks_count (int): æ€»å—æ•°
        total_bytes (int): æ€»å­—èŠ‚æ•°
    """
    try:
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(
                f"\n[{datetime.now().isoformat()}] Stream completed: {chunks_count} chunks, {total_bytes} bytes total\n"
            )
    except Exception as e:
        struct_logger.console_logger.error(
            f"å†™å…¥æµå¼å“åº”å®Œæˆä¿¡æ¯å¤±è´¥: {e}", file_path=file_path
        )


def _create_streaming_wrapper(original_iterator, request_id):
    """åˆ›å»ºæµå¼å“åº”åŒ…è£…å™¨ï¼Œç”¨äºæ‹¦æˆªå’Œè®°å½•æ¯ä¸ªæ•°æ®å—

    Args:
        original_iterator: åŸå§‹å“åº”è¿­ä»£å™¨
        request_id (str): è¯·æ±‚ID

    Returns:
        async generator: åŒ…è£…åçš„å¼‚æ­¥ç”Ÿæˆå™¨
    """
    chunks_count = 0
    total_bytes = 0
    streaming_log_file = None

    async def wrapped_iterator():
        nonlocal chunks_count, total_bytes, streaming_log_file
        start_stream_time = time.time()

        # ä»…åœ¨debugæ¨¡å¼ä¸‹åˆ›å»ºæµå¼å“åº”æ—¥å¿—æ–‡ä»¶
        if Config.is_debug_mode():
            _ensure_streaming_log_dir()
            streaming_log_file = _get_streaming_log_file_path(request_id)

            # åœ¨æ—¥å¿—ä¸­è®°å½•æ–‡ä»¶è·¯å¾„
            request_logger.info(f"ğŸŸ¢ [{request_id}] æµå¼å“åº”æ–‡ä»¶: {streaming_log_file}")

        try:
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨é€Ÿç‡é™åˆ¶è¿­ä»£å™¨
            if Config.local_dev.enable_streaming_response_rate_limit:
                request_logger.info(
                    f"ğŸš¦ [{request_id}] å¯ç”¨æµå¼å“åº”é€Ÿç‡é™åˆ¶ï¼ˆå‰10ä¸ªä¸é™åˆ¶ï¼‰"
                )
                iterator = create_rate_limited_iterator(original_iterator)
            else:
                iterator = original_iterator

            # ç»Ÿä¸€å¤„ç†æ‰€æœ‰chunk
            async for chunk in iterator:
                chunks_count += 1
                total_bytes += len(chunk)

                # ä»…åœ¨debugæ¨¡å¼ä¸‹å°†å—å†…å®¹å†™å…¥æ–‡ä»¶
                if Config.is_debug_mode() and streaming_log_file:
                    chunk_content = chunk.decode("utf-8", errors="ignore")
                    _write_chunk_to_file(
                        streaming_log_file, chunk_content, chunks_count, len(chunk)
                    )

                yield chunk

        finally:
            # æµå¼å“åº”å®Œæˆåè®°å½•æ€»æ—¶é—´å’Œè¯¦ç»†ä¿¡æ¯
            stream_process_time = (time.time() - start_stream_time) * 1000
            request_logger.info(
                f"ğŸŸ¢ [{request_id}] æµå¼å“åº”å®Œæˆ: å—æ•°={chunks_count}, æ€»å¤§å°={total_bytes}B, æµå¤„ç†æ—¶é—´={stream_process_time:.2f}ms"
            )

            # å¦‚æœæœ‰æ—¥å¿—æ–‡ä»¶ï¼Œè®°å½•æ–‡ä»¶å®Œæˆä¿¡æ¯
            if streaming_log_file and Config.is_debug_mode():
                _write_stream_completion_info(
                    streaming_log_file, chunks_count, total_bytes
                )

    return wrapped_iterator()


def handle_streaming_response(response, request_id, process_time):
    """å¤„ç†æµå¼å“åº”ï¼ŒåŒ…è£…å“åº”è¿­ä»£å™¨å¹¶è®°å½•åˆå§‹ä¿¡æ¯

    Args:
        response: FastAPIå“åº”å¯¹è±¡
        request_id (str): è¯·æ±‚ID
        process_time (float): åˆå§‹å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        Response: åŒ…è£…åçš„å“åº”å¯¹è±¡
    """
    # æ›¿æ¢åŸå§‹çš„å“åº”è¿­ä»£å™¨ä¸ºåŒ…è£…åçš„è¿­ä»£å™¨
    response.body_iterator = _create_streaming_wrapper(
        response.body_iterator, request_id
    )

    # è®°å½•æµå¼å“åº”çš„åˆå§‹ä¿¡æ¯
    response_info = {
        "status_code": response.status_code,
        "is_streaming": True,
        "initial_process_time_ms": f"{process_time:.2f}",
    }
    request_logger.info(f"ğŸŸ¢ [{request_id}] æµå¼è¯·æ±‚å¤„ç†ä¸­: {response_info}")

    return response
